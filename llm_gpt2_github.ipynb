{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1oaPFs9vejWzivQcRUrNoGU4ZcPUpYpqi",
      "authorship_tag": "ABX9TyORdaUuQzUEVfdy+7VTSQrY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ataerman/LLM-gpt2-/blob/main/llm_gpt2_github.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eh_AoodghI9V",
        "outputId": "2a41e49a-4a8c-403a-91c9-59c1011ff83f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken   #for tokenizer\n",
        "import tiktoken\n",
        "import importlib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Device name:\", torch.cuda.get_device_name(0))\n",
        "print(\"Device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6oo0rNrh3HG",
        "outputId": "67e2b2f9-6af6-4c12-e4d8-6278bc2073e4"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "Device name: NVIDIA A100-SXM4-40GB\n",
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#read data and create dataloader\n",
        "with open('/content/drive/MyDrive/Large Language Models/data/verdict.txt','r',encoding='utf-8') as f:\n",
        "  text_data=f.read()\n",
        "print(text_data[:20])\n",
        "print(len(text_data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KR_k5ilQiHoU",
        "outputId": "7fb1226b-c469-45f5-8194-792e11927fa3"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I HAD always thought\n",
            "20479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTDatasetV1(Dataset):\n",
        "  def __init__(self,txt,tokenizer,max_length,stride):\n",
        "    self.input_ids=[]\n",
        "    self.target_ids=[]\n",
        "\n",
        "    token_ids=tokenizer.encode(txt,allowed_special={'<|endoftext|>'})\n",
        "\n",
        "    for i in range(0,len(token_ids)-max_length,stride):\n",
        "      input_chunk=token_ids[i:i+max_length]\n",
        "      target_chunk=token_ids[i+1:i+max_length+1]\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "  def __getitem__(self,idx):\n",
        "    return self.input_ids[idx],self.target_ids[idx]\n",
        "def create_dataloader_v1(txt,batch_size=4,max_length=256,stride=128,shuffle=True,drop_last=True,num_workers=0):\n",
        "  tokenizer=tiktoken.get_encoding('gpt2')\n",
        "  dataset=GPTDatasetV1(txt,tokenizer,max_length,stride)\n",
        "\n",
        "  dataloader=DataLoader(\n",
        "      dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=shuffle,\n",
        "      drop_last=drop_last,\n",
        "      num_workers=num_workers\n",
        "  )\n",
        "  return dataloader\n",
        ""
      ],
      "metadata": {
        "id": "Qsv_Yd6ziYdb"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define GPT-2 model configuration (124M parameters version)\n",
        "GPT_CONFIG_124M = {\n",
        "    'vocab_size': 50257,          # Number of tokens in GPT-2's vocabulary\n",
        "    'context_length': 256,        # Maximum sequence length (window size)\n",
        "    'emb_dim': 768,               # Embedding dimension (hidden size)\n",
        "    'n_heads': 12,                # Number of attention heads\n",
        "    'n_layers': 12,               # Number of transformer blocks (layers)\n",
        "    'drop_rate': 0.1,             # Dropout rate for regularization\n",
        "    'qkv_bias': True             # Whether to use bias in Q, K, V projections\n",
        "}\n"
      ],
      "metadata": {
        "id": "vcwzv56pjWZd"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train and val loaders\n",
        "\n",
        "train_ratio=0.90\n",
        "split_idx=int(train_ratio*len(text_data))\n",
        "train_data=text_data[:split_idx]\n",
        "val_data=text_data[split_idx:]\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader=create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M['context_length'],\n",
        "    stride=GPT_CONFIG_124M['context_length'],\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader=create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M['context_length'],\n",
        "    stride=GPT_CONFIG_124M['context_length'],\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=0\n",
        ")\n",
        "print(\"Train loader:\")\n",
        "for x, y in train_loader:\n",
        "    print(x.shape, y.shape)\n",
        "\n",
        "print(\"\\nValidation loader:\")\n",
        "for x, y in val_loader:\n",
        "    print(x.shape, y.shape)\n",
        "\n",
        "print(len(train_loader))\n",
        "print(len(val_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUmCh9k0jrYH",
        "outputId": "4745ff09-c28d-4ae8-98df-2bc4d628c358"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "\n",
            "Validation loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "9\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_in,d_out,context_length,dropout,num_heads,qkv_bias=True):\n",
        "    super().__init__()\n",
        "    assert (d_out % num_heads==0),\\\n",
        "      'd_out must be divisible by num_heads'\n",
        "\n",
        "    self.d_out=d_out\n",
        "    self.num_heads=num_heads\n",
        "    self.head_dim=d_out//num_heads\n",
        "\n",
        "    self.W_query=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.W_key=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.W_value=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.out_proj=nn.Linear(d_out,d_out)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    self.register_buffer(\"mask\",torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
        "\n",
        "  def forward(self,x):\n",
        "    b,num_tokens,d_in=x.shape\n",
        "\n",
        "    keys=self.W_key(x)  #shape:(b,num_tokens,d_out)(2,4,6 (if num_heads==2))\n",
        "    queries=self.W_query(x)\n",
        "    values=self.W_value(x)\n",
        "\n",
        "    #(b,num_tokens,d_out)>(b,num_tokens,num_heads,head_dim)(2,4,6 > 2,4,2,3)\n",
        "    keys=keys.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "    queries=queries.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "    values=values.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "\n",
        "    keys=keys.transpose(1,2)   #(b,num_tokens,num_heads,head_dim)>(b,num_heads,num_tokens,head_dim)\n",
        "    values=values.transpose(1,2)\n",
        "    queries=queries.transpose(1,2)\n",
        "\n",
        "    attn_scores=queries @ keys.transpose(2,3) #(b,num_heads,num_tokens,head_dim) @ ((b,num_heads,head_dim,num_tokens))\n",
        "    #attn_score.shape=(b,num_heads,num_tokens,num_tokens)\n",
        "\n",
        "    mask_bool=self.mask.bool()[:num_tokens,:num_tokens] #Applying masking\n",
        "    attn_scores = attn_scores.masked_fill(mask_bool, -torch.inf)\n",
        "\n",
        "\n",
        "\n",
        "    attn_weights=torch.softmax(attn_scores/keys.shape[-1]**0.5,dim=-1) #applying scale and softmax\n",
        "    attn_weights=self.dropout(attn_weights)\n",
        "\n",
        "    context_vec=(attn_weights@values).transpose(1,2) #-->(b,num_tokens,num_heads,head_dim)\n",
        "    context_vec=context_vec.contiguous().view(b,num_tokens,self.d_out) #(b,num_tokens,self.d_out)\n",
        "    context_vec=self.out_proj(context_vec)\n",
        "\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "Zy-NbYovks_J"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #testing\n",
        "# d_in = d_out = 768\n",
        "# context_length = 8\n",
        "# num_heads = 12\n",
        "# dropout = 0.0\n",
        "# x = torch.randn(2, context_length, d_in)\n",
        "# attn = MultiHeadAttention(\n",
        "#     d_in=d_in,\n",
        "#     d_out=d_out,\n",
        "#     context_length=context_length,\n",
        "#     dropout=dropout,\n",
        "#     num_heads=num_heads,\n",
        "#     qkv_bias=True)\n",
        "# with torch.no_grad():\n",
        "#     out = attn(x)\n",
        "# print(\"Girdi boyutu :\", x.shape)\n",
        "# print(\"Çıktı boyutu :\", out.shape)\n"
      ],
      "metadata": {
        "id": "Uq2e6MIFlX_R"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self,emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps=1e-5\n",
        "    self.scale=nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift=nn.Parameter(torch.zeros(emb_dim))\n",
        "  def forward(self,x):\n",
        "    mean=x.mean(dim=-1,keepdim=True) #dim=-1 for columns keepdim = keep dimensions\n",
        "    var=x.var(dim=-1,keepdim=True,unbiased=False) #unbiased=False = // n    not n-1 for var\n",
        "    norm_x=(x-mean)/torch.sqrt(var+self.eps)\n",
        "    return self.scale*norm_x+self.shift\n",
        "class Gelu(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "  def forward(self,x):\n",
        "    return 0.5*x*(1+torch.tanh(torch.sqrt(torch.tensor(2.0/torch.pi))*(x+0.044715*torch.pow(x,3))))\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.layers=nn.Sequential(\n",
        "        nn.Linear(cfg['emb_dim'],4*cfg['emb_dim']),\n",
        "        Gelu(),\n",
        "        nn.Linear(4*cfg['emb_dim'],cfg['emb_dim'])\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "Lmcb43a7luKN"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.att=MultiHeadAttention(\n",
        "        d_in=cfg['emb_dim'],\n",
        "        d_out=cfg['emb_dim'],\n",
        "        context_length=cfg['context_length'],\n",
        "        num_heads=cfg['n_heads'],\n",
        "        dropout=cfg['drop_rate'],\n",
        "        qkv_bias=cfg['qkv_bias'])\n",
        "    self.ff=FeedForward(cfg)\n",
        "    self.norm1=LayerNorm(cfg['emb_dim'])\n",
        "    self.norm2=LayerNorm(cfg['emb_dim'])\n",
        "    self.drop_shortcut=nn.Dropout(cfg['drop_rate'])\n",
        "\n",
        "  def forward(self,x):\n",
        "    shortcut=x\n",
        "    x=self.norm1(x)\n",
        "    x=self.att(x)\n",
        "    x=self.drop_shortcut(x)\n",
        "    x=x+shortcut\n",
        "\n",
        "    shortcut=x\n",
        "    x=self.norm2(x)\n",
        "    x=self.ff(x)\n",
        "    x=self.drop_shortcut(x)\n",
        "    x=x+shortcut\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "BNMKPihkl7Sq"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb=nn.Embedding(cfg['vocab_size'],cfg['emb_dim'])\n",
        "    self.pos_emb=nn.Embedding(cfg['context_length'],cfg['emb_dim'])\n",
        "    self.drop_emb=nn.Dropout(cfg['drop_rate'])\n",
        "    self.trf_blocks=nn.Sequential(\n",
        "        *[TransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
        "    self.final_norm=LayerNorm(cfg['emb_dim'])\n",
        "    self.out_head=nn.Linear(cfg['emb_dim'],cfg['vocab_size'],bias=False)\n",
        "  def forward(self,in_idx):\n",
        "    batch_size,seq_length=in_idx.shape\n",
        "    tok_embeds=self.tok_emb(in_idx)\n",
        "    pos_embeds=self.pos_emb(torch.arange(seq_length,device=in_idx.device))\n",
        "    x=tok_embeds+pos_embeds\n",
        "    x=self.drop_emb(x)\n",
        "    x=self.trf_blocks(x)\n",
        "    x=self.final_norm(x)\n",
        "    logits=self.out_head(x)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "bovzhfg8mSOw"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=GPTModel(GPT_CONFIG_124M)\n",
        "model.eval()\n",
        "tokenizer = tiktoken.get_encoding('gpt2')\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # (batch_size, seq_len)\n",
        "    return encoded_tensor\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0)\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]  # last token prediction\n",
        "        if top_k is not None:\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1].unsqueeze(-1)\n",
        "            logits = torch.where(logits < min_val, torch.full_like(logits, float('-inf')), logits)\n",
        "\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "        if eos_id is not None:\n",
        "            if (idx_next == eos_id).all():\n",
        "                break\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "8E3P4_2Gmzeb"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Number of parameters: {total_params:,}\")\n",
        "#out_head.w=tok_emb.w so 162-38 almost 124M params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8Xj4LwJpPtz",
        "outputId": "4ca31a18-16c6-449a-c529-6f552f3ea2d0"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 162,447,360\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch,target_batch,model,device):\n",
        "  input_batch,target_batch=input_batch.to(device),target_batch.to(device)\n",
        "  logits=model(input_batch)\n",
        "  logits=logits.flatten(0,1)\n",
        "  targets=target_batch.flatten()\n",
        "  loss=torch.nn.functional.cross_entropy(logits,targets)\n",
        "  return loss\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float('nan')\n",
        "    if num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    with torch.no_grad():\n",
        "        for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "            if i < num_batches:\n",
        "                loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "                total_loss += loss.item()\n",
        "            else:\n",
        "                break\n",
        "    model.train()\n",
        "    return total_loss / num_batches"
      ],
      "metadata": {
        "id": "_P2dyz4OnWth"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_simple(model,train_loader,val_loader,optimizer,device,num_epochs,\n",
        "                        eval_freq,eval_iter,start_context,tokenizer):\n",
        "  train_losses,val_losses,track_tokens_seen=[],[],[]\n",
        "  tokens_seen,global_step=0,-1\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for input_batch,target_batch in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      loss=calc_loss_batch(input_batch,target_batch,model,device)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      tokens_seen+=input_batch.numel()\n",
        "      global_step +=1\n",
        "\n",
        "      if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "    generate_and_print_sample(model,tokenizer,device,start_context)\n",
        "  return train_losses, val_losses, track_tokens_seen\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()"
      ],
      "metadata": {
        "id": "-3d-TVSQnd83"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 100\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Note:\n",
        "# Uncomment the following code to show the execution time\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7co96ClvniIy",
        "outputId": "fd99ea9e-b078-4565-9a21-07e774110952"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 9.811, Val loss 9.936\n",
            "Ep 1 (Step 000005): Train loss 8.081, Val loss 8.300\n",
            "Every effort moves you,,, the,,,, the, the, the, the,,,,,, the, the,, the, the,, the,,, the, the,, the,, the,,, the, the\n",
            "Ep 2 (Step 000010): Train loss 6.693, Val loss 7.076\n",
            "Ep 2 (Step 000015): Train loss 6.069, Val loss 6.572\n",
            "Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and\n",
            "Ep 3 (Step 000020): Train loss 5.439, Val loss 6.493\n",
            "Ep 3 (Step 000025): Train loss 5.579, Val loss 6.430\n",
            "Every effort moves you I                                                 \n",
            "Ep 4 (Step 000030): Train loss 5.283, Val loss 6.447\n",
            "Ep 4 (Step 000035): Train loss 5.109, Val loss 6.392\n",
            "Every effort moves you, I was.                                              \n",
            "Ep 5 (Step 000040): Train loss 4.334, Val loss 6.286\n",
            "Every effort moves you.            \"I he was the fact--I had been.      \"I I was he was the and I had been.        \n",
            "Ep 6 (Step 000045): Train loss 4.029, Val loss 6.248\n",
            "Ep 6 (Step 000050): Train loss 3.727, Val loss 6.178\n",
            "Every effort moves you know the end up-rooms, and I felt--as he was a little.             \"I turned, in the donkey, and I saw that, and in the donkey, and his\n",
            "Ep 7 (Step 000055): Train loss 3.159, Val loss 6.112\n",
            "Ep 7 (Step 000060): Train loss 2.959, Val loss 6.078\n",
            "Every effort moves you know that Mrs.  \"I had to the fact with a little: \"Yes--and, and went on groping and m. \"Oh, and he had been--and I had a little, and it. \"Oh\n",
            "Ep 8 (Step 000065): Train loss 2.528, Val loss 6.121\n",
            "Ep 8 (Step 000070): Train loss 2.058, Val loss 6.100\n",
            "Every effort moves you know that mighty up-stream stroke. . . . . . . . . . . . . . . .  \"I turned, I was fitting that they should mourn him--I had longed to see the donkey, in the first\n",
            "Ep 9 (Step 000075): Train loss 1.813, Val loss 6.106\n",
            "Ep 9 (Step 000080): Train loss 1.363, Val loss 6.143\n",
            "Every effort moves you know,\" was one of the picture--I felt to see the tips of a self-confident moustache, I had been what a degree he had the same quality as his pictures--the quality of Jack's \"strong he had been his\n",
            "Ep 10 (Step 000085): Train loss 1.219, Val loss 6.173\n",
            "Every effort moves you know,\" was not that my hostess was \"interesting\": on that point I could have disarming, and went on groping and muddling; then I looked at the donkey again. I saw that, my eye fell on a small picture\n",
            "Ep 11 (Step 000090): Train loss 0.814, Val loss 6.243\n",
            "Ep 11 (Step 000095): Train loss 0.705, Val loss 6.292\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his\n",
            "Ep 12 (Step 000100): Train loss 0.499, Val loss 6.355\n",
            "Ep 12 (Step 000105): Train loss 0.448, Val loss 6.467\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 13 (Step 000110): Train loss 0.275, Val loss 6.473\n",
            "Ep 13 (Step 000115): Train loss 0.234, Val loss 6.499\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back the window-curtains, moved aside a _jardiniere_ full of\n",
            "Ep 14 (Step 000120): Train loss 0.177, Val loss 6.602\n",
            "Ep 14 (Step 000125): Train loss 0.151, Val loss 6.682\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 15 (Step 000130): Train loss 0.161, Val loss 6.745\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, when, on a later day, one might put it, married a rich widow, and established himself in\n",
            "Ep 16 (Step 000135): Train loss 0.137, Val loss 6.844\n",
            "Ep 16 (Step 000140): Train loss 0.113, Val loss 6.778\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back the window-curtains, and I turned, and down the room, stopping now\n",
            "Ep 17 (Step 000145): Train loss 0.091, Val loss 6.873\n",
            "Ep 17 (Step 000150): Train loss 0.130, Val loss 6.928\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 18 (Step 000155): Train loss 0.078, Val loss 6.890\n",
            "Ep 18 (Step 000160): Train loss 0.084, Val loss 6.954\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 19 (Step 000165): Train loss 0.064, Val loss 6.991\n",
            "Ep 19 (Step 000170): Train loss 0.050, Val loss 7.012\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 20 (Step 000175): Train loss 0.048, Val loss 7.063\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 21 (Step 000180): Train loss 0.053, Val loss 7.035\n",
            "Ep 21 (Step 000185): Train loss 0.044, Val loss 7.115\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 22 (Step 000190): Train loss 0.047, Val loss 7.198\n",
            "Ep 22 (Step 000195): Train loss 0.034, Val loss 7.166\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 23 (Step 000200): Train loss 0.039, Val loss 7.172\n",
            "Ep 23 (Step 000205): Train loss 0.044, Val loss 7.169\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 24 (Step 000210): Train loss 0.031, Val loss 7.287\n",
            "Ep 24 (Step 000215): Train loss 0.029, Val loss 7.243\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 25 (Step 000220): Train loss 0.023, Val loss 7.214\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 26 (Step 000225): Train loss 0.021, Val loss 7.211\n",
            "Ep 26 (Step 000230): Train loss 0.017, Val loss 7.261\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 27 (Step 000235): Train loss 0.016, Val loss 7.307\n",
            "Ep 27 (Step 000240): Train loss 0.025, Val loss 7.314\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 28 (Step 000245): Train loss 0.018, Val loss 7.346\n",
            "Ep 28 (Step 000250): Train loss 0.015, Val loss 7.363\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 29 (Step 000255): Train loss 0.010, Val loss 7.295\n",
            "Ep 29 (Step 000260): Train loss 0.010, Val loss 7.355\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 30 (Step 000265): Train loss 0.015, Val loss 7.393\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 31 (Step 000270): Train loss 0.022, Val loss 7.399\n",
            "Ep 31 (Step 000275): Train loss 0.010, Val loss 7.461\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\" \"I made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his\n",
            "Ep 32 (Step 000280): Train loss 0.010, Val loss 7.462\n",
            "Ep 32 (Step 000285): Train loss 0.009, Val loss 7.404\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 33 (Step 000290): Train loss 0.021, Val loss 7.451\n",
            "Ep 33 (Step 000295): Train loss 0.008, Val loss 7.444\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 34 (Step 000300): Train loss 0.005, Val loss 7.441\n",
            "Ep 34 (Step 000305): Train loss 0.013, Val loss 7.410\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 35 (Step 000310): Train loss 0.005, Val loss 7.440\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 36 (Step 000315): Train loss 0.005, Val loss 7.524\n",
            "Ep 36 (Step 000320): Train loss 0.006, Val loss 7.577\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 37 (Step 000325): Train loss 0.005, Val loss 7.490\n",
            "Ep 37 (Step 000330): Train loss 0.005, Val loss 7.477\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 38 (Step 000335): Train loss 0.004, Val loss 7.515\n",
            "Ep 38 (Step 000340): Train loss 0.004, Val loss 7.540\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 39 (Step 000345): Train loss 0.004, Val loss 7.516\n",
            "Ep 39 (Step 000350): Train loss 0.007, Val loss 7.513\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 40 (Step 000355): Train loss 0.007, Val loss 7.524\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 41 (Step 000360): Train loss 0.003, Val loss 7.569\n",
            "Ep 41 (Step 000365): Train loss 0.005, Val loss 7.585\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 42 (Step 000370): Train loss 0.003, Val loss 7.567\n",
            "Ep 42 (Step 000375): Train loss 0.003, Val loss 7.563\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 43 (Step 000380): Train loss 0.003, Val loss 7.569\n",
            "Ep 43 (Step 000385): Train loss 0.003, Val loss 7.582\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 44 (Step 000390): Train loss 0.002, Val loss 7.602\n",
            "Ep 44 (Step 000395): Train loss 0.002, Val loss 7.606\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 45 (Step 000400): Train loss 0.002, Val loss 7.607\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 46 (Step 000405): Train loss 0.002, Val loss 7.604\n",
            "Ep 46 (Step 000410): Train loss 0.002, Val loss 7.605\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 47 (Step 000415): Train loss 0.002, Val loss 7.607\n",
            "Ep 47 (Step 000420): Train loss 0.002, Val loss 7.614\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 48 (Step 000425): Train loss 0.002, Val loss 7.621\n",
            "Ep 48 (Step 000430): Train loss 0.002, Val loss 7.627\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 49 (Step 000435): Train loss 0.002, Val loss 7.634\n",
            "Ep 49 (Step 000440): Train loss 0.002, Val loss 7.641\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 50 (Step 000445): Train loss 0.002, Val loss 7.647\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 51 (Step 000450): Train loss 0.002, Val loss 7.654\n",
            "Ep 51 (Step 000455): Train loss 0.002, Val loss 7.661\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 52 (Step 000460): Train loss 0.002, Val loss 7.668\n",
            "Ep 52 (Step 000465): Train loss 0.002, Val loss 7.668\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 53 (Step 000470): Train loss 0.002, Val loss 7.667\n",
            "Ep 53 (Step 000475): Train loss 0.002, Val loss 7.668\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 54 (Step 000480): Train loss 0.001, Val loss 7.671\n",
            "Ep 54 (Step 000485): Train loss 0.001, Val loss 7.675\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 55 (Step 000490): Train loss 0.001, Val loss 7.684\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 56 (Step 000495): Train loss 0.001, Val loss 7.693\n",
            "Ep 56 (Step 000500): Train loss 0.001, Val loss 7.701\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 57 (Step 000505): Train loss 0.001, Val loss 7.704\n",
            "Ep 57 (Step 000510): Train loss 0.001, Val loss 7.706\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 58 (Step 000515): Train loss 0.001, Val loss 7.709\n",
            "Ep 58 (Step 000520): Train loss 0.001, Val loss 7.712\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 59 (Step 000525): Train loss 0.001, Val loss 7.716\n",
            "Ep 59 (Step 000530): Train loss 0.001, Val loss 7.721\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 60 (Step 000535): Train loss 0.001, Val loss 7.723\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 61 (Step 000540): Train loss 0.001, Val loss 7.726\n",
            "Ep 61 (Step 000545): Train loss 0.001, Val loss 7.721\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 62 (Step 000550): Train loss 0.001, Val loss 7.726\n",
            "Ep 62 (Step 000555): Train loss 0.001, Val loss 7.733\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 63 (Step 000560): Train loss 0.001, Val loss 7.743\n",
            "Ep 63 (Step 000565): Train loss 0.001, Val loss 7.754\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 64 (Step 000570): Train loss 0.001, Val loss 7.759\n",
            "Ep 64 (Step 000575): Train loss 0.001, Val loss 7.762\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 65 (Step 000580): Train loss 0.001, Val loss 7.763\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 66 (Step 000585): Train loss 0.001, Val loss 7.764\n",
            "Ep 66 (Step 000590): Train loss 0.001, Val loss 7.766\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 67 (Step 000595): Train loss 0.001, Val loss 7.769\n",
            "Ep 67 (Step 000600): Train loss 0.001, Val loss 7.773\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 68 (Step 000605): Train loss 0.001, Val loss 7.775\n",
            "Ep 68 (Step 000610): Train loss 0.001, Val loss 7.779\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 69 (Step 000615): Train loss 0.001, Val loss 7.784\n",
            "Ep 69 (Step 000620): Train loss 0.001, Val loss 7.788\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 70 (Step 000625): Train loss 0.001, Val loss 7.788\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 71 (Step 000630): Train loss 0.001, Val loss 7.792\n",
            "Ep 71 (Step 000635): Train loss 0.001, Val loss 7.800\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 72 (Step 000640): Train loss 0.001, Val loss 7.804\n",
            "Ep 72 (Step 000645): Train loss 0.001, Val loss 7.800\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 73 (Step 000650): Train loss 0.001, Val loss 7.778\n",
            "Ep 73 (Step 000655): Train loss 0.003, Val loss 7.789\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 74 (Step 000660): Train loss 0.001, Val loss 7.810\n",
            "Ep 74 (Step 000665): Train loss 0.001, Val loss 7.817\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 75 (Step 000670): Train loss 0.001, Val loss 7.821\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 76 (Step 000675): Train loss 0.001, Val loss 7.810\n",
            "Ep 76 (Step 000680): Train loss 0.001, Val loss 7.806\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 77 (Step 000685): Train loss 0.001, Val loss 7.823\n",
            "Ep 77 (Step 000690): Train loss 0.001, Val loss 7.838\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 78 (Step 000695): Train loss 0.001, Val loss 7.844\n",
            "Ep 78 (Step 000700): Train loss 0.001, Val loss 7.842\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 79 (Step 000705): Train loss 0.001, Val loss 7.839\n",
            "Ep 79 (Step 000710): Train loss 0.001, Val loss 7.839\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 80 (Step 000715): Train loss 0.001, Val loss 7.844\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 81 (Step 000720): Train loss 0.001, Val loss 7.848\n",
            "Ep 81 (Step 000725): Train loss 0.001, Val loss 7.853\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 82 (Step 000730): Train loss 0.001, Val loss 7.856\n",
            "Ep 82 (Step 000735): Train loss 0.001, Val loss 7.860\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 83 (Step 000740): Train loss 0.001, Val loss 7.865\n",
            "Ep 83 (Step 000745): Train loss 0.001, Val loss 7.870\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 84 (Step 000750): Train loss 0.001, Val loss 7.872\n",
            "Ep 84 (Step 000755): Train loss 0.001, Val loss 7.874\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 85 (Step 000760): Train loss 0.001, Val loss 7.875\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 86 (Step 000765): Train loss 0.001, Val loss 7.877\n",
            "Ep 86 (Step 000770): Train loss 0.001, Val loss 7.883\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 87 (Step 000775): Train loss 0.001, Val loss 7.887\n",
            "Ep 87 (Step 000780): Train loss 0.001, Val loss 7.891\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 88 (Step 000785): Train loss 0.001, Val loss 7.892\n",
            "Ep 88 (Step 000790): Train loss 0.001, Val loss 7.889\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 89 (Step 000795): Train loss 0.001, Val loss 7.889\n",
            "Ep 89 (Step 000800): Train loss 0.001, Val loss 7.892\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 90 (Step 000805): Train loss 0.001, Val loss 7.899\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 91 (Step 000810): Train loss 0.001, Val loss 7.904\n",
            "Ep 91 (Step 000815): Train loss 0.001, Val loss 7.905\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 92 (Step 000820): Train loss 0.001, Val loss 7.906\n",
            "Ep 92 (Step 000825): Train loss 0.001, Val loss 7.898\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 93 (Step 000830): Train loss 0.001, Val loss 7.892\n",
            "Ep 93 (Step 000835): Train loss 0.001, Val loss 7.893\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 94 (Step 000840): Train loss 0.001, Val loss 7.904\n",
            "Ep 94 (Step 000845): Train loss 0.001, Val loss 7.917\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 95 (Step 000850): Train loss 0.001, Val loss 7.925\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 96 (Step 000855): Train loss 0.001, Val loss 7.929\n",
            "Ep 96 (Step 000860): Train loss 0.001, Val loss 7.933\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 97 (Step 000865): Train loss 0.001, Val loss 7.936\n",
            "Ep 97 (Step 000870): Train loss 0.001, Val loss 7.936\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 98 (Step 000875): Train loss 0.001, Val loss 7.936\n",
            "Ep 98 (Step 000880): Train loss 0.001, Val loss 7.937\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 99 (Step 000885): Train loss 0.001, Val loss 7.938\n",
            "Ep 99 (Step 000890): Train loss 0.001, Val loss 7.933\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 100 (Step 000895): Train loss 0.001, Val loss 7.930\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Training completed in 2.37 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(\"loss-plot.pdf\")\n",
        "    plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "OOcvmF2anobM",
        "outputId": "a9dbb8ae-fbcb-4dc0-b63f-4a51d9918382"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Save checkpoint (Google Drive'a) ---\n",
        "checkpoint = {\n",
        "    'epoch': num_epochs,  # İstersen son epoch'u da kaydedebilirsin\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "}\n",
        "\n",
        "torch.save(checkpoint, '/content/drive/MyDrive/gptmodel_checkpoint.pth')\n",
        "print(\"Checkpoint saved to Drive!\")\n",
        "\n",
        "# --- Save full model (Google Drive'a) ---\n",
        "torch.save(model, '/content/drive/MyDrive/gptmodel_full.pth')\n",
        "print(\"Full model saved to Drive!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r755hylfn9Qp",
        "outputId": "2dbdc4c5-0e5b-4e89-eb56-80b8c6fccc41"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved to Drive!\n",
            "Full model saved to Drive!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zUnT7XvMsBh0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}